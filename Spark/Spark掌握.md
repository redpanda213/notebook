# Spark掌握

## 数据倾斜处理方案

首先我们要知道什么是数据倾斜，为什么会发生数据倾斜？？

**什么是数据倾斜呢？**对spark来讲，spark是一种大数据系统，而大数据量不可怕，数据倾斜更可怕。

数据倾斜指，在并行处理数据集时，某一部分，如spark的一个partition，这一部分的数据显著多余其他partition，导致此partition的处理速度成为整个数据集处理的瓶颈，也就是一种木桶效应。

**数据倾斜是如何造成的呢?**也就是问，为什么某个partition的数据会特别多呢？一个是数据的分散度不够，导致大量的数据集中到了一台或者几台机器上计算，在一次shuffle过程就会产生数据倾斜。

![这里写图片描述](https://img-blog.csdn.net/20170905164538706?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDAzOTkyOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

举个例子，就是淘宝订单，比如广东省的双11订单数和新疆和比，广东的订单量是新疆的100倍，那么在统计不同城市的订单情况时，一做groupby操作，就会导致数据倾斜，所有相同key如广东省的数据都被拉到一个节点上。

**那我们有什么办法解决这个问题呢？**我们呢可以从3个方面去考虑，**一是业务逻辑，二是程序层面，三是调参层面**。详细来讲呢，首先说业务逻辑，还是拿刚才那个例子来讲，

- 从业务和数据上解决数据倾斜

这个和计算平台无关，无论是spark还是hadoop。因为是由数据本事的原因。

我们的方法，可以用有损的方法，***如过滤掉原始数据中的异常数据**，如ip为0的数据。无损的方法呢，**对分布不均匀的数据，单独进行操作计算。也可以对key先做一层hash，讲数据打散让它并行度变大，再汇集**。

还有等等数据预处理的方法。

- 程序层面

通过编写的代码来调整造成的数据倾斜

- 调参层面

即**调整spark或者hadoop的许多自带的参数**，和机制调节i数据倾斜，合理使用，也可以解决大部分问题。

### 具体优化方法

- mapjoin方法
- 设置rdd压缩
- 合理设置driver的内存
- Spark Sql中的优化和Hive类似，参考hive

1、调整并行度分散同一个Task的不同Key：即调整shuffle时的并行度，使原本被分配到同一个Task的不同Key分配到不同Task上处理，则可降低原Task所需处理的数据量。

2、自定义Partitioner：默认为HashPartitioner，将原本被分配到同一个Task的不同Key分配到不同Task，

3、将Reduce side join转变为 Map side join，通过spark的broadcast机制，转化。避免shuffle，从而消除shuffle带来的数据倾斜。

4、为skew的key增加随机前、后缀

5、大表随机添加 N 种随机前缀，小表扩大 N 倍:如果出现数据倾斜的 Key 比较多，上一种方法将这些大量的倾斜 Key 分拆出来，意义不大（很难一个 Key 一个 Key 都加上后缀）。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大 N 倍），可以看到 RDD2 扩大了 N 倍了，再和加完前缀的大数据做笛卡尔积。

## shuffle过程

Spark shuffle 处于一个宽依赖，可以实现类似混洗的功能，将相同的Key分发至同一个Reducer上进行处理。

## 聚合类算子

### 有哪些

### 避免用什么类型的算子

避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。可以大大减少性能开销

## spark on yarn执行流程

 